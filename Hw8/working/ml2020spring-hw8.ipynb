{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8 - Sequence-to-sequence\n",
    "\n",
    "# Sequence-to-Sequence 介紹\n",
    "- 大多數常見的 **sequence-to-sequence (seq2seq) model** 為 **encoder-decoder model**，主要由兩個部分組成，分別是 **Encoder** 和 **Decoder**，而這兩個部分則大多使用 **recurrent neural network (RNN)** 來實作，主要是用來解決輸入和輸出的長度不一樣的情況\n",
    "- **Encoder** 是將**一連串**的輸入，如文字、影片、聲音訊號等，編碼為**單個向量**，這單個向量可以想像為是整個輸入的抽象表示，包含了整個輸入的資訊\n",
    "- **Decoder** 是將 Encoder 輸出的單個向量逐步解碼，**一次輸出一個結果**，直到將最後目標輸出被產生出來為止，每次輸出會影響下一次的輸出，一般會在開頭加入 \"< BOS >\" 來表示開始解碼，會在結尾輸出 \"< EOS >\" 來表示輸出結束\n",
    "\n",
    "\n",
    "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
    "\n",
    "# 作業介紹\n",
    "- 英文翻譯中文\n",
    "  - 輸入： 一句英文 （e.g.\t\ttom is a student .） \n",
    "  - 輸出： 中文翻譯 （e.g. \t\t湯姆 是 個 學生 。）\n",
    "\n",
    "- TODO\n",
    "  - Teachering Forcing 的功用: 嘗試不用 Teachering Forcing 做訓練\n",
    "  - 實作 Attention Mechanism\n",
    "  - 實作 Beam Search\n",
    "  - 實作 Schedule Sampling\n",
    "\n",
    "# 资料下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 判斷是用 CPU 還是 GPU 執行運算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 资料结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义资料的转换\n",
    "- 将不同长度的答案拓展到相同长度，以便训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里定义了一个 transform 来作 padding 很巧妙\n",
    "\n",
    "class LabelTransform(object):\n",
    "    def __init__(self,size,pad):\n",
    "        self.size = size\n",
    "        self.pad = pad\n",
    "    \n",
    "    def __call__(self,label):\n",
    "        label = np.pad(label,(0,(self.size - label.shape[0])),mode='constant',constant_values=self.pad)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 Dataset\n",
    "- Data (出自manythings 的 cmn-eng):\n",
    "  - 訓練資料：18000句\n",
    "  - 檢驗資料：  500句\n",
    "  - 測試資料： 2636句\n",
    "\n",
    "- 資料預處理:\n",
    "  - 英文：\n",
    "    - 用 subword-nmt 套件將word轉為subword\n",
    "    - 建立字典：取出標籤中出現頻率高於定值的subword\n",
    "  - 中文：\n",
    "    - 用 jieba 將中文句子斷詞\n",
    "    - 建立字典：取出標籤中出現頻率高於定值的詞\n",
    "  - 特殊字元： < PAD >, < BOS >, < EOS >, < UNK > \n",
    "    - < PAD >  ：無意義，將句子拓展到相同長度\n",
    "    - < BOS >  ：Begin of sentence, 開始字元\n",
    "    - < EOS >  ：End of sentence, 結尾字元\n",
    "    - < UNK > ：單字沒有出現在字典裡的字\n",
    "  - 將字典裡每個 subword (詞) 用一個整數表示，分為英文和中文的字典，方便之後轉為 one-hot vector   \n",
    "\n",
    "- 處理後的檔案:\n",
    "  - 字典：\n",
    "    - int2word_*.json: 將整數轉為文字\n",
    "    ![int2word_en.json](https://i.imgur.com/31E4MdZ.png)\n",
    "    - word2int_*.json: 將文字轉為整數\n",
    "    ![word2int_en.json](https://i.imgur.com/9vI4AS1.png)\n",
    "    - $*$ 分為英文（en）和中文（cn）\n",
    "  \n",
    "  - 訓練資料:\n",
    "    - 不同語言的句子用 TAB ('\\t') 分開\n",
    "    - 字跟字之間用空白分開\n",
    "    ![data](https://i.imgur.com/nSH1fH4.png)\n",
    "\n",
    "\n",
    "- 在將答案傳出去前，在答案開頭加入 \"< BOS >\" 符號，並於答案結尾加入 \"< EOS >\" 符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "class EN2CNDataset(data.Dataset):\n",
    "    def __init__(self,root,max_output_len,set_name):\n",
    "        self.root = root\n",
    "        \n",
    "        # get dictionary\n",
    "        self.word2int_cn,self.int2word_cn = self.get_dictionary('cn')\n",
    "        self.word2int_en,self.int2word_en = self.get_dictionary('en')\n",
    "        \n",
    "        self.cn_vocab_size = len(self.word2int_cn)\n",
    "        self.en_vocab_size = len(self.word2int_en)\n",
    "        \n",
    "        # load data\n",
    "        self.data = []\n",
    "        with open(os.path.join(self.root,f'{set_name}.txt'),'r') as f:\n",
    "            for l in f:\n",
    "                self.data.append(l)\n",
    "        print(f'Loading dataset:{set_name}.txt, len:{len(self.data)}')\n",
    "        \n",
    "        # set transform\n",
    "        self.transform = LabelTransform(max_output_len,self.word2int_en['<PAD>'])\n",
    "        \n",
    "    def get_dictionary(self,language):\n",
    "        '''\n",
    "        get dictionary of expected language\n",
    "        '''\n",
    "        with open(os.path.join(self.root,f'word2int_{language}.json'),'r') as f:\n",
    "            word2int = json.load(f)\n",
    "        with open(os.path.join(self.root,f'int2word_{language}.json'),'r') as f:\n",
    "            int2word = json.load(f)\n",
    "        return word2int, int2word\n",
    "    \n",
    "    def split_en_cn_sentences(self,sentence):\n",
    "        '''\n",
    "        split en and cn sentences in a sentence\n",
    "        '''\n",
    "        sentences = re.split('[\\t\\n]',sentence)\n",
    "        sentences = list(filter(None,sentences)) # ignore ''\n",
    "        assert len(sentences) == 2\n",
    "        return sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        '''\n",
    "        get item of desired index\n",
    "        return en,cn - en and cn sentence, LongTensor, same length\n",
    "        '''\n",
    "        # split en cn sentences\n",
    "        sentences = self.split_en_cn_sentences(self.data[index])\n",
    "        \n",
    "        # special words\n",
    "        BOS = self.word2int_en['<BOS>']\n",
    "        EOS = self.word2int_en['<EOS>']\n",
    "        UNK = self.word2int_en['<UNK>']\n",
    "        \n",
    "        # get en and cn vectors, all begin with <BOS>, endwith <EOS>\n",
    "        en, cn = [BOS], [BOS]\n",
    "        \n",
    "        # get en vector\n",
    "        # first, split the sentence into subwords and change them into int\n",
    "        sentence = sentences[0]\n",
    "        sentence = re.split(' ',sentence)\n",
    "        sentence = list(filter(None,sentence)) # ignore ''\n",
    "#         print('En sentence: ',sentence)\n",
    "        for word in sentence:\n",
    "            en.append(self.word2int_en.get(word,UNK))\n",
    "        en.append(EOS)\n",
    "        \n",
    "        # get cn vector\n",
    "        # first, split the sentence into subwords and change them into int\n",
    "        sentence = sentences[1] # cn sentence\n",
    "        sentence = re.split(' ',sentence)\n",
    "        sentence = list(filter(None,sentence)) # ignore ''\n",
    "#         print('Cn sentence: ',sentence)\n",
    "        for word in sentence:\n",
    "            cn.append(self.word2int_cn.get(word,UNK))\n",
    "        cn.append(EOS)\n",
    "        \n",
    "        # change en and cn to array\n",
    "        en,cn = np.asarray(en),np.asarray(cn)\n",
    "        \n",
    "        # make the sentences having the same length\n",
    "        en,cn = self.transform(en), self.transform(cn)\n",
    "        \n",
    "        return en,cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架构\n",
    "\n",
    "## Encoder\n",
    "- seq2seq模型的編碼器為RNN。 對於每個輸入，，**Encoder** 會輸出**一個向量**和**一個隱藏狀態(hidden state)**，並將隱藏狀態用於下一個輸入，換句話說，**Encoder** 會逐步讀取輸入序列，並輸出單個矢量（最終隱藏狀態）\n",
    "- 參數:\n",
    "  - en_vocab_size 是英文字典的大小，也就是英文的 subword 的個數\n",
    "  - emb_dim 是 embedding 的維度，主要將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用，可以使用預先訓練好的 word embedding，如 Glove 和 word2vector\n",
    "  - hid_dim 是 RNN 輸出和隱藏狀態的維度\n",
    "  - n_layers 是 RNN 要疊多少層\n",
    "  - dropout 是決定有多少的機率會將某個節點變為 0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不使用\n",
    "- Encoder 的輸入和輸出:\n",
    "  - 輸入: \n",
    "    - 英文的整數序列 e.g. 1, 28, 29, 205, 2\n",
    "  - 輸出: \n",
    "    - outputs: 最上層 RNN 全部的輸出，可以用 Attention 再進行處理\n",
    "    - hidden: 每層最後的隱藏狀態，將傳遞到 Decoder 進行解碼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,en_vocab_size,emb_dim,hid_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(en_vocab_size,emb_dim)\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "            # input = [batch, seq,    ] = [batch, seq, emb_dim]\n",
    "        # h_0 = [n_layers*2, batch, hid_dim]\n",
    "        # output = [seq_len, batch, 2*hid_dim]\n",
    "        # h_n = [n_layers*2, batch, hid_dim]\n",
    "        self.rnn = nn.GRU(emb_dim,hid_dim,n_layers,dropout=dropout,batch_first=True,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        # inputs = [*], output = [*,emb_dim]\n",
    "        embedding = self.embedding(inputs)\n",
    "        output, hidden = self.rnn(self.dropout(embedding))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "- **Decoder** 是另一個 RNN，在最簡單的 seq2seq decoder 中，僅使用 **Encoder** 每一層最後的隱藏狀態來進行解碼，而這最後的隱藏狀態有時被稱為 “content vector”，因為可以想像它對整個前文序列進行編碼， 此 “content vector” 用作 **Decoder** 的**初始**隱藏狀態， 而 **Encoder** 的輸出通常用於 Attention Mechanism\n",
    "- 參數\n",
    "  - cn_vocab_size 是英文字典的大小，也就是中文的 subword 的個數\n",
    "  - emb_dim 是 embedding 的維度，是用來將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用，可以使用預先訓練好的 word embedding，如 Glove 和 word2vector\n",
    "  - hid_dim 是 RNN 輸出和隱藏狀態的維度\n",
    "  - output_dim 是最終輸出的維度，一般來說是將 hid_dim 轉到 one-hot vector 的單詞向量\n",
    "  - n_layers 是 RNN 要疊多少層\n",
    "  - dropout 是決定有多少的機率會將某個節點變為0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不用\n",
    "  - isatt 是來決定是否使用 Attention Mechanism\n",
    "\n",
    "- Decoder 的輸入和輸出:\n",
    "  - 輸入:\n",
    "    - 前一次解碼出來的單詞的整數表示\n",
    "  - 輸出:\n",
    "    - hidden: 根據輸入和前一次的隱藏狀態，現在的隱藏狀態更新的結果\n",
    "    - output: 每個字有多少機率是這次解碼的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,cn_vocab_size,emb_dim,hid_dim,n_layers,dropout,isatt):\n",
    "        super().__init__()\n",
    "        self.cn_vocab_size = cn_vocab_size\n",
    "        self.hid_dim = hid_dim * 2\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(cn_vocab_size,emb_dim)\n",
    "        self.isatt = isatt\n",
    "        self.attention = Attention(hid_dim)\n",
    "        # 如果使用 Attention Mechanism 會使得輸入維度變化，請在這裡修改\n",
    "        # e.g. Attention 接在輸入後面會使得維度變化，所以輸入維度改為\n",
    "        self.input_dim = emb_dim + hid_dim * 2 if isatt else emb_dim\n",
    "#         self.input_dim = emb_dim\n",
    "        self.rnn = nn.GRU(self.input_dim,self.hid_dim,self.n_layers,dropout=dropout,batch_first=True)\n",
    "        # ???? what is this? - it`s a fc\n",
    "        self.embedding2vocab1 = nn.Linear(self.hid_dim,self.hid_dim*2)\n",
    "        self.embedding2vocab2 = nn.Linear(self.hid_dim*2,self.hid_dim*4)\n",
    "        self.embedding2vocab3 = nn.Linear(self.hid_dim*4,self.cn_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,inputs,hidden, encoder_outputs):\n",
    "        # inputs = [batch size, vocab size]\n",
    "        # hidden = [batch size, n layers * directions, hid dim]\n",
    "        # Decoder 只會是單向，所以 directions=1\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        # embedded = [batch size, 1, emb dim]\n",
    "        \n",
    "        if self.isatt:\n",
    "            attn = self.attention(encoder_outputs,hidden)\n",
    "            # TODO: 在這裡決定如何使用 Attention，e.g. 相加 或是 接在後面， 請注意維度變化\n",
    "            embedded = torch.cat([embedded,attn],dim=2)\n",
    "        output, hidden = self.rnn(embedded,hidden)\n",
    "        # output = [batch size, 1, hid dim]\n",
    "        # hidden = [num_layers, batch size, hid dim]\n",
    "        \n",
    "        # 将 RNN 的输出转为每个词出现的机率\n",
    "        output = self.embedding2vocab1(output.squeeze(1))\n",
    "        output = self.embedding2vocab2(output)\n",
    "        prediction = self.embedding2vocab3(output)\n",
    "        # prediction = [batch size, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "- 當輸入過長，或是單獨靠 “content vector” 無法取得整個輸入的意思時，用 Attention Mechanism 來提供 **Decoder** 更多的資訊\n",
    "- 主要是根據現在 **Decoder hidden state** ，去計算在 **Encoder outputs** 中，那些與其有較高的關係，根據關系的數值來決定該傳給 **Decoder** 那些額外資訊 \n",
    "- 常見 Attention 的實作是用 Neural Network / Dot Product 來算 **Decoder hidden state** 和 **Encoder outputs** 之間的關係，再對所有算出來的數值做 **softmax** ，最後根據過完 **softmax** 的值對 **Encoder outputs** 做 **weight sum**\n",
    "\n",
    "- TODO:\n",
    "實作 Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        # encoder_outputs = [batch size, sequence len, hid dim * directions] = [60,50,1024]\n",
    "        # decoder_hidden = [num_layers, batch size, hid dim] = [3,60,1024]\n",
    "        # 一般來說是取 Decoder 最後一層的 hidden state 來做 attention\n",
    "\n",
    "        decoder_last_hidden = decoder_hidden[-1,:,:]\n",
    "        decoder_last_hidden = decoder_last_hidden.unsqueeze(1)\n",
    "        decoder_last_hidden = decoder_last_hidden.transpose(1,2)\n",
    "\n",
    "        alpha = encoder_outputs.bmm(decoder_last_hidden).squeeze(2)\n",
    "        alpha = torch.nn.functional.softmax(alpha,dim=1).unsqueeze(1)\n",
    "        attention = alpha.bmm(encoder_outputs)\n",
    "        return attention  # [batch, 1, hid_dim * 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "- 由 **Encoder** 和 **Decoder** 組成\n",
    "- 接收輸入並傳給 **Encoder** \n",
    "- 將 **Encoder** 的輸出傳給 **Decoder**\n",
    "- 不斷地將 **Decoder** 的輸出傳回 **Decoder** ，進行解碼  \n",
    "- 當解碼完成後，將 **Decoder** 的輸出傳回 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self,inputs,target,teacher_forcing_ratio):\n",
    "        # inputs = [batch size, input len, vocab size]\n",
    "        # target = [batch size, target len, vocab size]\n",
    "        # teacher_forcing_ratio 是有多少机率使用正确答案来训练\n",
    "        batch_size = target.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        vocab_size = self.decoder.cn_vocab_size\n",
    "        \n",
    "        # 准备一个存储空间来存储输出\n",
    "        outputs = torch.zeros(batch_size,target_len,vocab_size).to(self.device)\n",
    "        # 将输入放入 Encoder\n",
    "        encoder_outputs, hidden = self.encoder(inputs)\n",
    "        # Encoder 最后的隐藏层 hidden state 用来初始化 Decoder\n",
    "        # encoder outputs 主要使用在 Attention\n",
    "        # 因为 Encoder 是双向的RNN 所以需要将同一层两个方向的 hidden state 接在一起 ------ ？？？？\n",
    "        # hidden = [num_layers * directions, batch size, hid dim] \n",
    "        #          --> [num_layers, directions, batch size, hid dim] \n",
    "        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\n",
    "        hidden = torch.cat((hidden[:,-2,:,:], hidden[:,-1,:,:]),dim=2)\n",
    "        # 取的 <BOS> token\n",
    "        inputs = target[:,0]\n",
    "        preds = []\n",
    "        for t in range(1,target_len):\n",
    "            output, hidden = self.decoder(inputs,hidden,encoder_outputs)\n",
    "            outputs[:,t] = output\n",
    "            # 决定是否用正确答案来做训练\n",
    "            teacher_force = random.random() <= teacher_forcing_ratio\n",
    "            # 取出机率最大的单词\n",
    "            top1 = output.argmax(1)\n",
    "            # 如果是 teacher force 则用正解训练 反之用自己预测的单词做预测\n",
    "            inputs = target[:,t] if teacher_force and t < target_len else top1\n",
    "            preds.append(top1.unsqueeze(1))\n",
    "        preds = torch.cat(preds, 1)\n",
    "        return outputs, preds\n",
    "    \n",
    "    # 这里为什么要给 test 单独一个 inference 而不是共用 train 的 forward 呢 ？？？ 因为 scheduled teaching的缘故\n",
    "    def inference(self,inputs,target):\n",
    "        #########\n",
    "        # TODO  # 我准备做 beam_size=3 的 beam search\n",
    "        #########\n",
    "        # 在这里实施 Beam Search\n",
    "        # 此函数的 batch size = 1\n",
    "        # input = [batch size, input len, vocab size]\n",
    "        # target = [batch size, target len, vocab size]\n",
    "        batch_size = inputs.shape[0]\n",
    "        input_len = inputs.shape[1]\n",
    "        vocab_size = self.decoder.cn_vocab_size\n",
    "\n",
    "        #---------------------------------------------\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        #---------------------------------------------\n",
    "        \n",
    "        \n",
    "        # 准备一个存储空间来存储输出\n",
    "        outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\n",
    "        # 将输入放入 Encoder\n",
    "        encoder_outputs, hidden = self.encoder(inputs)\n",
    "        # Encoder 最后的隐藏层 hidden state 用来初始化 Decoder\n",
    "        # encoder ouputs 主要用在 Attention\n",
    "        # 因为 Encoder 是双向的 RNN 所以需要将同一层两个方向的 hidden state 接在一起\n",
    "        # hidden = [num_layers * directions, batch_size, hid_dim] \n",
    "        #           --> [num_layers, direction, batch_size, hid_dim]\n",
    "        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\n",
    "        hidden = torch.cat((hidden[:,-2,:,:],hidden[:,-1,:,:]),dim=2)\n",
    "        # 取的 <BOS> token\n",
    "        inputs = target[:,0]\n",
    "        preds = []\n",
    "        for t in range(1, input_len):\n",
    "            output,hidden = self.decoder(inputs,hidden,encoder_outputs)\n",
    "            # 将预测结果存起来\n",
    "            outputs[:,t] = output\n",
    "            # 取出机率最大的单词\n",
    "#             top1 = output.argmax(1)\n",
    "            top1 = output.argmax(3) # beam size == 3\n",
    "            inputs = top1\n",
    "            preds.append(top1.unsqueeze(1))\n",
    "        preds = torch.cat(preds,1)\n",
    "        return outputs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "- 基本操作:\n",
    "  - 儲存模型\n",
    "  - 載入模型\n",
    "  - 建構模型\n",
    "  - 將一連串的數字還原回句子\n",
    "  - 計算 BLEU score\n",
    "  - 迭代 dataloader\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, store_model_path, step):\n",
    "    torch.save(model.state_dict(),f'{store_model_path}/model_{step}.ckpt')\n",
    "    print(f'Saving model {store_model_path}/model_{step}.ckpt')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, load_model_path):\n",
    "    model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\n",
    "    print(f'Load model from {load_model_path}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config,en_vocab_size,cn_vocab_size):\n",
    "    encoder = Encoder(en_vocab_size,config.emb_dim,config.hid_dim,config.n_layers,config.dropout)\n",
    "    decoder = Decoder(cn_vocab_size,config.emb_dim,config.hid_dim,config.n_layers,config.dropout,\n",
    "                      config.attention)\n",
    "    model = Seq2Seq(encoder,decoder,device)\n",
    "    print(model)\n",
    "    # 构建 optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    print('\\n',optimizer)\n",
    "    if config.load_model:\n",
    "        model = load_model(model,config.load_model_path)\n",
    "    model = model.to(device)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数字转句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2sentence(outputs, int2word):\n",
    "    sentences = []\n",
    "    for tokens in outputs:\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            word = int2word[str(int(token))]\n",
    "            if word == '<EOS>':\n",
    "                break\n",
    "            sentence.append(word)\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 BLUE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "\n",
    "def computebleu(sentences,targets):\n",
    "    score = 0\n",
    "    assert len(sentences) == len(targets)\n",
    "    \n",
    "    def cut_token(sentence):\n",
    "        tmp = []\n",
    "        for token in sentence:\n",
    "            # what is this???\n",
    "            if token == '<UNK>' or token.isdigit() or len(bytes(token[0],encoding='utf-8'))==1:\n",
    "                tmp.append(token)\n",
    "            else:\n",
    "                tmp += [word for word in token]\n",
    "        return tmp\n",
    "    \n",
    "    for sentence,target in zip(sentences,targets):\n",
    "        sentence,target= cut_token(sentence),cut_token(target)\n",
    "        score += sentence_bleu([target],sentence,weights=[1,0,0,0])\n",
    "    \n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinit_iter(dataloader):\n",
    "    it = iter(dataloader)\n",
    "    while True:\n",
    "        try:\n",
    "            ret = next(it)\n",
    "            yield ret\n",
    "        except StopIteration:\n",
    "            it = iter(dataloader)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schedule sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########\n",
    "\n",
    "# 請在這裡直接 return 0 來取消 Teacher Forcing\n",
    "# 請在這裡實作 schedule_sampling 的策略\n",
    "\n",
    "# linear decay\n",
    "global_schedule_ratio = 1\n",
    "# def schedule_sampling():\n",
    "#     global global_schedule_ratio\n",
    "#     global_schedule_ratio -= 1.0/config.num_steps\n",
    "#     return global_schedule_ratio\n",
    "\n",
    "# always uses teacher forcing\n",
    "# def schedule_sampling():\n",
    "#     return 1\n",
    "\n",
    "# inverse sigmoid decay\n",
    "global_step = 0\n",
    "# def schedule_sampling():\n",
    "#     global global_step\n",
    "#     global_step += 1\n",
    "#     x = (global_step - config.num_steps/2)/1000\n",
    "#     return 1 - F.sigmoid(torch.tensor(x,dtype=torch.float)).item()\n",
    "\n",
    "# exponential decay\n",
    "def schedule_sampling():\n",
    "    global global_step\n",
    "    global_step += 1  \n",
    "    return 1 - np.log(global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练步骤\n",
    "## 训练\n",
    "- 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,train_iter,loss_function,total_steps,summary_steps):\n",
    "    '''\n",
    "    return model, optimizer, losses\n",
    "    '''\n",
    "    # set train in train model\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    losses = []\n",
    "    loss_sum = 0.0\n",
    "    for step in range(summary_steps):\n",
    "        sources,targets = next(train_iter)\n",
    "        sources,targets = sources.to(device),targets.to(device)\n",
    "        outputs,preds = model(sources,targets,schedule_sampling())\n",
    "        # targets 的第一个 toekn 是 <BOS> 所以忽略\n",
    "        outputs = outputs[:,1:].reshape(-1,outputs.size(2))\n",
    "        targets = targets[:,1:].reshape(-1)\n",
    "        loss = loss_function(outputs,targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1) # why ???? 处理梯度爆炸\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        if (step+1) % 5 == 0:\n",
    "            loss_sum = loss_sum/5\n",
    "            print('\\r','train [{}] loss {:.3f}, Perplexity {:.3f}    '.format(\n",
    "                total_steps + step + 1,   loss_sum,   np.exp(loss_sum)\n",
    "                ),end=' ')\n",
    "            losses.append(loss_sum)\n",
    "            loss_sum = 0.0\n",
    "        \n",
    "    return model,optimizer,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检验/测试\n",
    "- 防止训练发生 overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataloader,loss_function):\n",
    "    model.eval()\n",
    "    loss_sum, bleu_score = 0.0,0.0\n",
    "    n = 0\n",
    "    result = []\n",
    "    for sources, targets in dataloader:\n",
    "        sources,targets = sources.to(device),targets.to(device)\n",
    "        batch_size = sources.size(0)\n",
    "        outputs, preds = model.inference(sources, targets)\n",
    "        # targets 的第一个 token 是 <BOS> 所以忽略\n",
    "        outputs = outputs[:,1:].reshape(-1, outputs.size(2))\n",
    "        targets = targets[:,1:].reshape(-1)\n",
    "        \n",
    "        loss = loss_function(outputs,targets)\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        # 将预测结果转为文字\n",
    "        targets = targets.view(sources.size(0),-1)\n",
    "        preds = tokens2sentence(preds,dataloader.dataset.int2word_cn)\n",
    "        sources = tokens2sentence(sources,dataloader.dataset.int2word_en)\n",
    "        targets = tokens2sentence(targets,dataloader.dataset.int2word_cn)\n",
    "        for source, pred, target in zip(sources, preds, targets):\n",
    "            result.append((source,pred,target))\n",
    "        # 计算 Bleu Score\n",
    "        bleu_score += computebleu(preds, targets)\n",
    "        \n",
    "        n += batch_size\n",
    "        \n",
    "    return loss_sum / len(dataloader), bleu_score/n, result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练流程\n",
    "- 先训练 再检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(config):\n",
    "    start = time.time()\n",
    "    # 准备训练资料\n",
    "    train_dataset = EN2CNDataset(config.data_path,config.max_output_len,'training')\n",
    "    train_loader = data.DataLoader(train_dataset,batch_size=config.batch_size,shuffle=True)\n",
    "    train_iter = infinit_iter(train_loader)\n",
    "    # 准备检验资料\n",
    "    val_dataset = EN2CNDataset(config.data_path,config.max_output_len,'validation')\n",
    "    val_loader = data.DataLoader(val_dataset,batch_size=1)\n",
    "    # 构建模型\n",
    "    model, optimizer = build_model(config,train_dataset.en_vocab_size,train_dataset.cn_vocab_size)  \n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0) # ？？？ 是 ignore bias 的 grad 吗\n",
    "\n",
    "    # 训练过程\n",
    "    train_loss, val_losses, bleu_scores = [],[],[]\n",
    "    total_steps = 0\n",
    "    while total_steps < config.num_steps:\n",
    "        # 训练模型\n",
    "        model, optimizer, losses = train(model,optimizer,train_iter,loss_function,total_steps,\n",
    "             config.summary_steps)\n",
    "        train_loss += losses\n",
    "        # 检验模型\n",
    "        val_loss, bleu_score,result = test(model,val_loader,loss_function)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        \n",
    "        total_steps += config.summary_steps\n",
    "        print('\\r','val [{}] loss {:.3f}, Perplexity: {:.3f}, bleu score: {:.3f}, used {} seconds      '.format(\n",
    "                total_steps, val_loss, np.exp(val_loss), bleu_score, int(time.time()-start)\n",
    "            ))\n",
    "        \n",
    "        # 储存模型和结果\n",
    "        if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\n",
    "            save_model(model,config.store_model_path,total_steps)\n",
    "            with open(f'{config.store_model_path}/output_{total_steps}.txt','w') as f:\n",
    "                for l in result:\n",
    "                    print(l,file=f)\n",
    "    \n",
    "    return train_loss, val_losses, bleu_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process(config):\n",
    "    # 准备测试资料\n",
    "    test_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'testing')\n",
    "    test_loader = data.DataLoader(test_dataset,batch_size=1)\n",
    "    # 建构模型\n",
    "    model, optimizer = build_model(config,test_dataset.en_vocab_size,test_dataset.cn_vocab_size)\n",
    "    print('Finish build model')\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval()\n",
    "    # 测试模型\n",
    "    test_loss, bleu_score, result = test(model, test_loader, loss_function)\n",
    "    # 储存结果\n",
    "    with open(f'./test_output.txt','w') as f:\n",
    "        for line in result:\n",
    "            print(line, file=f)\n",
    "            \n",
    "    return test_loss, bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "- 实验的参数设定表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 60\n",
    "        self.emb_dim = 256\n",
    "        self.hid_dim = 512\n",
    "        self.n_layers = 3\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 0.00005\n",
    "        # 最后输出句子的最大长度 \n",
    "        self.max_output_len = 50\n",
    "        # 总训练次数\n",
    "        self.num_steps = 12000\n",
    "        # 训练多少次后需要存储模型\n",
    "        self.store_steps = 900\n",
    "        # 训练多少次后需要检验是否有 overfitting\n",
    "        self.summary_steps = 300\n",
    "        # 是否需要载入模型\n",
    "        self.load_model = False\n",
    "        # 存储模型的位置\n",
    "        self.store_model_path = '.'\n",
    "        # 载入模型的位置\n",
    "        self.load_model_path = f'{self.store_model_path}/model_{self.num_steps}'\n",
    "        # 资料存放的位置\n",
    "        self.data_path = '../input/hw8-data/cmn-eng'\n",
    "        # 是否使用 Attention Mechanism\n",
    "        self.attention = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "- 载入参数\n",
    "- 进行训练 or 推论\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'batch_size': 60, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'learning_rate': 5e-05, 'max_output_len': 50, 'num_steps': 12000, 'store_steps': 900, 'summary_steps': 300, 'load_model': False, 'store_model_path': '.', 'load_model_path': './model_12000', 'data_path': '../input/hw8-data/cmn-eng', 'attention': True} \n",
      "\n",
      "Loading dataset:training.txt, len:18000\n",
      "Loading dataset:validation.txt, len:500\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(3922, 256)\n",
      "    (rnn): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3805, 256)\n",
      "    (attention): Attention()\n",
      "    (rnn): GRU(1280, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "    (embedding2vocab1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (embedding2vocab2): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (embedding2vocab3): Linear(in_features=4096, out_features=3805, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c3f28040af8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-a75676273eb5>\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         model, optimizer, losses = train(model,optimizer,train_iter,loss_function,total_steps,\n\u001b[0;32m---> 20\u001b[0;31m              config.summary_steps)\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 检验模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-08dbf06186f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter, loss_function, total_steps, summary_steps)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# why ???? 处理梯度爆炸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = Configuration()\n",
    "    print('config:\\n',vars(config),'\\n')\n",
    "    train_losses, val_losses, bleu_scores = train_process(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'batch_size': 60, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'learning_rate': 5e-05, 'max_output_len': 50, 'num_steps': 12000, 'store_steps': 900, 'summary_steps': 300, 'load_model': False, 'store_model_path': '.', 'load_model_path': './model_12000', 'data_path': '../input/hw8-data/cmn-eng', 'attention': True} \n",
      "\n",
      "Loading dataset:testing.txt, len:2636\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(3922, 256)\n",
      "    (rnn): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3805, 256)\n",
      "    (attention): Attention()\n",
      "    (rnn): GRU(1280, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "    (embedding2vocab1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (embedding2vocab2): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (embedding2vocab3): Linear(in_features=4096, out_features=3805, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "Finish build model\n",
      "> <ipython-input-21-6b77f1160fec>(65)inference()\n",
      "-> outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\n",
      "(Pdb) l\n",
      " 60  \t        pdb.set_trace()\n",
      " 61  \t        #---------------------------------------------\n",
      " 62  \t\n",
      " 63  \t\n",
      " 64  \t        # 准备一个存储空间来存储输出\n",
      " 65  ->\t        outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\n",
      " 66  \t        # 将输入放入 Encoder\n",
      " 67  \t        encoder_outputs, hidden = self.encoder(inputs)\n",
      " 68  \t        # Encoder 最后的隐藏层 hidden state 用来初始化 Decoder\n",
      " 69  \t        # encoder ouputs 主要用在 Attention\n",
      " 70  \t        # 因为 Encoder 是双向的 RNN 所以需要将同一层两个方向的 hidden state 接在一起\n",
      "(Pdb) l 55\n",
      " 50  \t        # 在这里实施 Beam Search\n",
      " 51  \t        # 此函数的 batch size = 1\n",
      " 52  \t        # input = [batch size, input len, vocab size]\n",
      " 53  \t        # target = [batch size, target len, vocab size]\n",
      " 54  \t        batch_size = inputs.shape[0]\n",
      " 55  \t        input_len = inputs.shape[1]\n",
      " 56  \t        vocab_size = self.decoder.cn_vocab_size\n",
      " 57  \t\n",
      " 58  \t        #---------------------------------------------\n",
      " 59  \t        import pdb\n",
      " 60  \t        pdb.set_trace()\n",
      "(Pdb) batch_size,input_len,vocab_size\n",
      "(1, 50, 3805)\n",
      "(Pdb) l\n",
      " 61  \t        #---------------------------------------------\n",
      " 62  \t\n",
      " 63  \t\n",
      " 64  \t        # 准备一个存储空间来存储输出\n",
      " 65  ->\t        outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\n",
      " 66  \t        # 将输入放入 Encoder\n",
      " 67  \t        encoder_outputs, hidden = self.encoder(inputs)\n",
      " 68  \t        # Encoder 最后的隐藏层 hidden state 用来初始化 Decoder\n",
      " 69  \t        # encoder ouputs 主要用在 Attention\n",
      " 70  \t        # 因为 Encoder 是双向的 RNN 所以需要将同一层两个方向的 hidden state 接在一起\n",
      " 71  \t        # hidden = [num_layers * directions, batch_size, hid_dim]\n",
      "(Pdb) n\n",
      "> <ipython-input-21-6b77f1160fec>(67)inference()\n",
      "-> encoder_outputs, hidden = self.encoder(inputs)\n",
      "(Pdb) n\n",
      "> <ipython-input-21-6b77f1160fec>(73)inference()\n",
      "-> hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\n",
      "(Pdb) l\n",
      " 68  \t        # Encoder 最后的隐藏层 hidden state 用来初始化 Decoder\n",
      " 69  \t        # encoder ouputs 主要用在 Attention\n",
      " 70  \t        # 因为 Encoder 是双向的 RNN 所以需要将同一层两个方向的 hidden state 接在一起\n",
      " 71  \t        # hidden = [num_layers * directions, batch_size, hid_dim]\n",
      " 72  \t        #           --> [num_layers, direction, batch_size, hid_dim]\n",
      " 73  ->\t        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\n",
      " 74  \t        hidden = torch.cat((hidden[:,-2,:,:],hidden[:,-1,:,:]),dim=2)\n",
      " 75  \t        # 取的 <BOS> token\n",
      " 76  \t        inputs = target[:,0]\n",
      " 77  \t        preds = []\n",
      " 78  \t        for t in range(1, input_len):\n",
      "(Pdb) encoder_outputs.shape,hidden.shape\n",
      "(torch.Size([1, 50, 1024]), torch.Size([6, 1, 512]))\n",
      "(Pdb) n\n",
      "> <ipython-input-21-6b77f1160fec>(74)inference()\n",
      "-> hidden = torch.cat((hidden[:,-2,:,:],hidden[:,-1,:,:]),dim=2)\n",
      "(Pdb) n\n",
      "> <ipython-input-21-6b77f1160fec>(76)inference()\n",
      "-> inputs = target[:,0]\n",
      "(Pdb) encoder_outputs.shape,hidden.shape\n",
      "(torch.Size([1, 50, 1024]), torch.Size([3, 1, 1024]))\n",
      "(Pdb) n\n",
      "> <ipython-input-21-6b77f1160fec>(77)inference()\n",
      "-> preds = []\n",
      "(Pdb) inputs.shape\n",
      "torch.Size([1])\n",
      "(Pdb) inputs\n",
      "tensor([1])\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d2a230a0f647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     config.load_model = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'test loss: {test_loss}, bleu_score: {bleu_score}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-ed14e2ab9228>\u001b[0m in \u001b[0;36mtest_process\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 测试模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 储存结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./test_output.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e8609546aa00>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader, loss_function)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# targets 的第一个 token 是 <BOS> 所以忽略\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-6b77f1160fec>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, inputs, target)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# 取的 <BOS> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-6b77f1160fec>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, inputs, target)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# 取的 <BOS> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 在执行 test 之前 需要在 config 中设定要载入的模型位置\n",
    "if __name__ == '__main__':\n",
    "    config = Configuration()\n",
    "#     config.load_model = True\n",
    "    print('config:\\n',vars(config),'\\n')\n",
    "    test_loss, bleu_score = test_process(config)\n",
    "    print(f'test loss: {test_loss}, bleu_score: {bleu_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图形化训练过程\n",
    "## 以图表呈现训练的loss的变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('次数')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以图表呈现检验的loss变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('次数')\n",
    "plt.ylabel('loss')\n",
    "plt.title('validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU scroe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(bleu_scores)\n",
    "plt.xlabel('次数')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.title('BLEU score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
