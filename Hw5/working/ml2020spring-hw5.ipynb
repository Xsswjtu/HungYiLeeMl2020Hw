{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils define","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_path_labels(path):\n    imgnames = os.listdir(path)\n    imgnames.sort()\n    imgpathes = []\n    imglabels = []\n    for img in imgnames:\n        imgpathes.append(os.path.join(path,img))\n        imglabels.append(int(img.split('_')[0]))\n    return imgpathes,imglabels\n\ndef normalize(image):\n    return (image - image.min())/(image.max() - image.min())\n\ndef compute_saliency_maps(x,y,model,device):\n    model.eval()\n    model = model.to(device)\n    x = x.to(device)\n    y = y.to(device)\n    x.requires_grad_()\n    y_pred = model(x)\n    criterion = torch.nn.CrossEntropyLoss()\n    loss = criterion(y_pred,y)\n    loss.backward()\n    saliencies = x.grad.abs().detach().cpu()\n    saliencies = torch.stack([normalize(item) for item in saliencies])\n    return saliencies\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # input [3,128,128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  # [64,128,128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),  # [64,64,64]\n\n            nn.Conv2d(64, 128, 3, 1, 1),  # [128,64,64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),  # [128,32,32]\n\n            nn.Conv2d(128, 256, 3, 1, 1),  # [256,32,32]\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),  # [256,16,16]\n\n            nn.Conv2d(256, 512, 3, 1, 1),  # [512,16,16]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),  # [512,8,8]\n\n            nn.Conv2d(512, 512, 3, 1, 1),  # [512,8,8]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),  # [512,4,4]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512 * 4 * 4, 1024),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(512, 11)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)\n#         import pdb\n#         pdb.set_trace()\n        out = out.reshape(out.shape[0], -1)\n        return self.fc(out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset define","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\nclass FoodDataset(Dataset):\n    def __init__(self, pathes, labels, model):\n        self.pathes = pathes\n        self.labels = labels\n        trainTransform = transforms.Compose([\n            transforms.Resize(size=(128, 128)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n        ])\n        evalTransform = transforms.Compose([\n            transforms.Resize(size=(128, 128)),\n            transforms.ToTensor(),\n        ])\n        self.transform = trainTransform if model == 'train' else evalTransform\n\n    def __len__(self):\n        return len(self.pathes)\n\n    def __getitem__(self, index):\n        X = Image.open(self.pathes[index])\n        X = self.transform(X)\n        y = self.labels[index]\n        return X,y\n\n    def getbatch(self,indices):\n        images,labels = [],[]\n        for index in indices:\n            X,y = self.__getitem__(index)\n            images.append(X)\n            labels.append(y)\n        return torch.stack(images),torch.tensor(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Global define","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = '../input/hw3-model/model_hw3.pth'\nmodel = Classifier().to(device)\nmodel.load_state_dict(torch.load(model_path,map_location=device))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saliency Maps","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dir = '../input/ml2020spring-hw3/food-11/'\n\n# select images to get saliencies\nimg_indices = [83, 4218, 4707, 8598]\n\n# getting selected images\nimgpathes, imglabels = get_path_labels(os.path.join(dataset_dir,'training'))\ntrain_dataset = FoodDataset(imgpathes,imglabels,'train')\nimages,labels = train_dataset.getbatch(img_indices)\n\n# computing saliency maps\nsaliencies = compute_saliency_maps(images,labels,model,device)\n\nfig,axes = plt.subplots(2,len(images),figsize=(15,8))\nfor row,target in enumerate([images,saliencies]):\n    for column,img in enumerate(target):\n        axes[row][column].imshow(img.permute(1,2,0).detach())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看到，我的模型并没有很好的找到目标的特征。它把注意力集中再目标的边缘，这可能就是我的模型效果不够好的原因。","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"下面我来看看，CNN中的filter到底看到了些什么，还有什么样的图片最能激活filter。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_activations = None #定义一个全局变量，用于将目标filter的输出结果抽出来\n\ndef filter_explaination(x,model,cnnid,filterid,device,iteration,lr):\n    '''\n    该函数用于解释CNN中的filter行为\n    cnnid, filterid - 指定要分析的是第几层layer的第几个filter\n    输出fiter activations和filter visulization\n    '''\n    # 将model和x移动到device上\n    model = model.to(device)\n    x = x.to(device)\n    \n    #set model in eval model\n    model.eval() \n    \n    # define a hook to save FILTER OUTPUT in layer_activations\n    def hook(model,input,output):\n        global layer_activations\n        layer_activations = output\n\n    # 挂钩子\n    hook_handle = model.cnn[cnnid].register_forward_hook(hook)\n    \n    # forward model\n    model(x)\n    \n    # 现在抽取了指定层layer的所有filter，现在只要特定的filter\n    filter_activations = layer_activations[:,filterid,:,:]\n    \n    # 我们已经获取了activation map，我们只是要将它画出来，所以可以将它从graph中detach出来，并转换成为一个cpu tensor\n    filter_activations = filter_activations.detach().cpu()\n    \n    # 现在我们来找到最大程度activate filter的图片，可以从random noise的图片开始找，也可以从指定图片开始找\n    # 要找到可以最大程度activate filter的图片，需要对x做grad descent\n    x.requires_grad_()\n    optimizer = optim.Adam([x],lr=lr)\n    for it in range(iteration):\n        optimizer.zero_grad() # 不要忘记zero grad\n        model(x)\n        \n        # 定义一个优化目标，这里用fiter输出的sum\n        # 之所以加上负号，是因为要找最大值\n        # 之所以没有用filter_activations这个变量，是因为它已经不在graph里了，无法计算grad\n        objective = -layer_activations[:,filterid,:,:].sum()\n    \n        objective.backward()\n        optimizer.step()\n        \n        print(f'iter: {it+1}/{iteration} objective: {objective}',end='\\r')\n    \n    # filter_visualization只是用来图形化，所以可以从graph detach掉，并转换为一个cpu tensor\n    filter_visualization = x.detach().cpu().squeeze()[0]\n    \n    # 摘钩子\n    hook_handle.remove()\n    \n    return filter_activations,filter_visualization\n\nfilter_activations,filter_visualization = filter_explaination(images,model,cnnid=8,filterid=0,\n                                                              device=device,iteration=100,lr=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the layer_activations\nimages,labels = train_dataset.getbatch(img_indices)\nfig,axes = plt.subplots(2,len(images),figsize=(15,8))\nfor i,img in enumerate(images):\n    axes[0][i].imshow(img.permute(1,2,0).detach())\nfor i,img in enumerate(filter_activations):\n    axes[1][i].imshow(img)\n\nplt.figure()\nplt.imshow(normalize(filter_visualization.permute(1,2,0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lime\nLime是一个现成的套件，只要实现两个函数，就可以用lime来解释模型的行为","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(input):\n    # input: numpy array, (batches, height, width, channels)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.to(device)\n    model.eval()\n    \n    # 需要先将input转成pytorch tensor (batches, channels, height, width)\n    input = torch.FloatTensor(input).permute(0,3,1,2)\n    \n    input = input.to(device)\n\n    output = model(input)\n    \n    return output.detach().cpu().numpy()\n\nfrom skimage.segmentation import slic\nfrom lime import lime_image\n\ndef segmentation(input):\n    # 利用 skimage 提供的 segmentation 将图片分成 100 块\n    return slic(input,n_segments=100,compactness=1,sigma=1)\n\nimg_indices = [83, 4218, 4707, 8598]\n# img_indices = [i+1 for i in img_indices]\n\nimages, labels = train_dataset.getbatch(img_indices)\n\n# ************************************\n# 我的模型太烂了，lime只能解释最重要的6种label，不打个补丁无法跑啊...\n# ************************************\nlabels = torch.tensor([2, 9, 2, 2])\n\n# 让实验reproducible\nnp.random.seed(16)\n\nfig,axes = plt.subplots(1,4,figsize=(15,8))\n\nfor idx, (image, label) in enumerate(zip(images.permute(0,2,3,1).numpy(),labels)):\n    x = image.astype(np.double)\n    # lime 這個套件要吃 numpy array\n\n    explainer = lime_image.LimeImageExplainer()                                                                                                                              \n    explaination = explainer.explain_instance(image=x, classifier_fn=predict, segmentation_fn=segmentation)\n    # 基本上只要提供給 lime explainer 兩個關鍵的 function，事情就結束了\n    # classifier_fn 定義圖片如何經過 model 得到 prediction\n    # segmentation_fn 定義如何把圖片做 segmentation\n    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explain_instance#lime.lime_image.LimeImageExplainer.explain_instance\n\n    lime_img, mask = explaination.get_image_and_mask(                                                                                                                         \n                                label=label.item(),                                                                                                                           \n                                positive_only=False,                                                                                                                         \n                                hide_rest=False,                                                                                                                             \n                                num_features=11,                                                                                                                              \n                                min_weight=0.05                                                                                                                              \n                            )\n    # 把 explainer 解釋的結果轉成圖片\n    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=get_image_and_mask#lime.lime_image.ImageExplanation.get_image_and_mask\n    \n    axes[idx].imshow(lime_img)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}