{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataReader import DataReader\n",
    "\n",
    "dr = DataReader(hours=9)\n",
    "X,y = dr.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X,(X**2),(X**3)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    X_norm = (X-mu)/std\n",
    "    return X_norm,mu,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.5)\n",
    "X_train_norm,mu,std = normal(X_train)\n",
    "X_test_norm = (X_test-mu)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LinearRegressionModel import LinearRegreesionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegreesionModel(lr=0.01,lbd=100,epoches=10000)\n",
    "model.fit(X_train_norm,y_train,X_test_norm,y_test)\n",
    "train_loss_history,test_loss_history,grad_history = model.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_history)\n",
    "plt.plot(test_loss_history)\n",
    "plt.legend(['train','test'])\n",
    "\n",
    "train_loss_history[-1],test_loss_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用全部数据集训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_norm,mu,std = normal(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1000 train loss:18.562529369360234 test loss:0\n",
      "epoch:2000 train loss:17.895551159745757 test loss:0\n",
      "epoch:3000 train loss:17.7088683932879 test loss:0\n",
      "epoch:4000 train loss:17.634971117323335 test loss:0\n",
      "epoch:5000 train loss:17.600974807429004 test loss:0\n",
      "epoch:6000 train loss:17.584067830103375 test loss:0\n",
      "epoch:7000 train loss:17.575259074881554 test loss:0\n",
      "epoch:8000 train loss:17.57052032411726 test loss:0\n",
      "epoch:9000 train loss:17.567906926941408 test loss:0\n",
      "epoch:10000 train loss:17.566434971288068 test loss:0\n",
      "epoch:11000 train loss:17.565590141287768 test loss:0\n",
      "epoch:12000 train loss:17.565096762154273 test loss:0\n",
      "epoch:13000 train loss:17.564803933046377 test loss:0\n",
      "epoch:14000 train loss:17.564627492361332 test loss:0\n",
      "epoch:15000 train loss:17.564519680860265 test loss:0\n",
      "epoch:16000 train loss:17.56445294932156 test loss:0\n",
      "epoch:17000 train loss:17.564411156423965 test loss:0\n",
      "epoch:18000 train loss:17.56438470302111 test loss:0\n",
      "epoch:19000 train loss:17.564367799403026 test loss:0\n",
      "epoch:20000 train loss:17.564356906869005 test loss:0\n",
      "epoch:21000 train loss:17.564349835693342 test loss:0\n",
      "epoch:22000 train loss:17.564345215428883 test loss:0\n",
      "epoch:23000 train loss:17.564342179485013 test loss:0\n",
      "epoch:24000 train loss:17.564340174770983 test loss:0\n",
      "epoch:25000 train loss:17.56433884534956 test loss:0\n",
      "epoch:26000 train loss:17.56433796047578 test loss:0\n",
      "epoch:27000 train loss:17.56433736959619 test loss:0\n",
      "epoch:28000 train loss:17.56433697392328 test loss:0\n",
      "epoch:29000 train loss:17.564336708315803 test loss:0\n",
      "epoch:30000 train loss:17.564336529633856 test loss:0\n",
      "epoch:31000 train loss:17.564336409200536 test loss:0\n",
      "epoch:32000 train loss:17.564336327890487 test loss:0\n",
      "epoch:33000 train loss:17.564336272911945 test loss:0\n",
      "epoch:34000 train loss:17.564336235687733 test loss:0\n",
      "epoch:35000 train loss:17.564336210453924 test loss:0\n",
      "epoch:36000 train loss:17.564336193329535 test loss:0\n",
      "epoch:37000 train loss:17.564336181696863 test loss:0\n",
      "epoch:38000 train loss:17.564336173787517 test loss:0\n",
      "epoch:39000 train loss:17.56433616840528 test loss:0\n",
      "epoch:40000 train loss:17.564336164739874 test loss:0\n",
      "epoch:41000 train loss:17.56433616224186 test loss:0\n",
      "epoch:42000 train loss:17.564336160538314 test loss:0\n",
      "epoch:43000 train loss:17.564336159375824 test loss:0\n",
      "epoch:44000 train loss:17.56433615858209 test loss:0\n",
      "epoch:45000 train loss:17.564336158039833 test loss:0\n",
      "epoch:46000 train loss:17.564336157669192 test loss:0\n",
      "epoch:47000 train loss:17.56433615741572 test loss:0\n",
      "epoch:48000 train loss:17.564336157242295 test loss:0\n",
      "epoch:49000 train loss:17.56433615712358 test loss:0\n",
      "epoch:50000 train loss:17.564336157042295 test loss:0\n",
      "epoch:51000 train loss:17.5643361569866 test loss:0\n",
      "epoch:52000 train loss:17.56433615694844 test loss:0\n",
      "epoch:53000 train loss:17.56433615692227 test loss:0\n",
      "epoch:54000 train loss:17.56433615690433 test loss:0\n",
      "epoch:55000 train loss:17.56433615689201 test loss:0\n",
      "epoch:56000 train loss:17.564336156883556 test loss:0\n",
      "epoch:57000 train loss:17.56433615687776 test loss:0\n",
      "epoch:58000 train loss:17.564336156873775 test loss:0\n",
      "epoch:59000 train loss:17.56433615687104 test loss:0\n",
      "epoch:60000 train loss:17.564336156869157 test loss:0\n",
      "epoch:61000 train loss:17.564336156867864 test loss:0\n",
      "epoch:62000 train loss:17.564336156866975 test loss:0\n",
      "epoch:63000 train loss:17.56433615686636 test loss:0\n",
      "epoch:64000 train loss:17.56433615686594 test loss:0\n",
      "epoch:65000 train loss:17.564336156865657 test loss:0\n",
      "epoch:66000 train loss:17.564336156865444 test loss:0\n",
      "epoch:67000 train loss:17.564336156865313 test loss:0\n",
      "epoch:68000 train loss:17.564336156865224 test loss:0\n",
      "epoch:69000 train loss:17.56433615686516 test loss:0\n",
      "epoch:70000 train loss:17.56433615686511 test loss:0\n",
      "epoch:71000 train loss:17.564336156865085 test loss:0\n",
      "epoch:72000 train loss:17.564336156865057 test loss:0\n",
      "epoch:73000 train loss:17.56433615686505 test loss:0\n",
      "epoch:74000 train loss:17.564336156865036 test loss:0\n",
      "epoch:75000 train loss:17.564336156865032 test loss:0\n",
      "epoch:76000 train loss:17.564336156865025 test loss:0\n",
      "epoch:77000 train loss:17.56433615686502 test loss:0\n",
      "epoch:78000 train loss:17.56433615686502 test loss:0\n",
      "epoch:79000 train loss:17.56433615686502 test loss:0\n",
      "epoch:80000 train loss:17.56433615686502 test loss:0\n",
      "epoch:81000 train loss:17.56433615686501 test loss:0\n",
      "epoch:82000 train loss:17.564336156865014 test loss:0\n",
      "epoch:83000 train loss:17.56433615686501 test loss:0\n",
      "epoch:84000 train loss:17.56433615686501 test loss:0\n",
      "epoch:85000 train loss:17.56433615686501 test loss:0\n",
      "epoch:86000 train loss:17.564336156865014 test loss:0\n",
      "epoch:87000 train loss:17.56433615686501 test loss:0\n",
      "epoch:88000 train loss:17.56433615686501 test loss:0\n",
      "epoch:89000 train loss:17.56433615686501 test loss:0\n",
      "epoch:90000 train loss:17.564336156865014 test loss:0\n",
      "epoch:91000 train loss:17.564336156865007 test loss:0\n",
      "epoch:92000 train loss:17.564336156865014 test loss:0\n",
      "epoch:93000 train loss:17.56433615686501 test loss:0\n",
      "epoch:94000 train loss:17.564336156865014 test loss:0\n",
      "epoch:95000 train loss:17.56433615686501 test loss:0\n",
      "epoch:96000 train loss:17.56433615686501 test loss:0\n",
      "epoch:97000 train loss:17.56433615686501 test loss:0\n",
      "epoch:98000 train loss:17.564336156865018 test loss:0\n",
      "epoch:99000 train loss:17.564336156865014 test loss:0\n",
      "epoch:100000 train loss:17.564336156865014 test loss:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_norm,y)\n",
    "train_loss_history,test_loss_history,grad_history = model.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.564336156865014"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_history[200:])\n",
    "\n",
    "train_loss_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('theta.npy',model.get_parameters())\n",
    "\n",
    "np.save('mu.npy',mu)\n",
    "\n",
    "np.save('sigma.npy',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.load('theta.npy')\n",
    "mu = np.load('mu.npy')\n",
    "std = np.load('sigma.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>48</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "      <td>67</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>73</td>\n",
       "      <td>79</td>\n",
       "      <td>82</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>73</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>115</td>\n",
       "      <td>45</td>\n",
       "      <td>107</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      2     3     4     5     6     7     8     9     10\n",
       "0     21    21    20    20    19    19    19    18    17\n",
       "1    1.7   1.7   1.7   1.7   1.7   1.7   1.7   1.7   1.8\n",
       "2   0.39  0.36  0.36   0.4  0.53  0.55  0.34  0.31  0.23\n",
       "3   0.16  0.24  0.22  0.27  0.27  0.26  0.27  0.29   0.1\n",
       "4    1.3   1.3   1.3   1.3   1.4   1.6   1.2   1.1   0.9\n",
       "5     17    14    13    14    18    21   8.9   9.4     5\n",
       "6     18    16    14    15    20    23    10    10   5.8\n",
       "7     32    31    31    26    16    12    27    20    26\n",
       "8     62    50    44    39    38    32    48    36    25\n",
       "9     33    39    39    25    18    18    17     9     4\n",
       "10     0     0     0     0     0     0     0     0     0\n",
       "11    83    85    87    87    86    85    78    81    80\n",
       "12     2   1.8   1.8   1.8   2.1   2.6     2   2.3   2.4\n",
       "13   1.8   1.9   1.9     2     2     2     2     2   1.9\n",
       "14    58    53    67    59    59    73    79    82   104\n",
       "15    57    44    73    44    56   115    45   107   103\n",
       "16   1.4   1.3   1.5   1.4   1.6   1.6   1.2   1.8   2.3\n",
       "17     1   0.9   0.9   0.9   1.2   0.7     1   0.6   1.8"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv',header=None)\n",
    "df[df=='NR'] = 0\n",
    "df.drop([0,1],axis=1,inplace=True)\n",
    "df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.astype(np.float32).reshape(240,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.concatenate((test_data,(test_data**2),(test_data**3)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (test_data-mu)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.concatenate((np.ones((240,1),dtype=np.float32),test_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = test_data@theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({'id':[f'id_{i}' for i in range(240)],'value':p.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
