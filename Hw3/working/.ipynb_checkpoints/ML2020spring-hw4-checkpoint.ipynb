{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_file_label = '../input/ml2020spring-hw4/training_label.txt'\n",
    "train_file_nolabel = '../input/ml2020spring-hw4/training_nolabel.txt'\n",
    "test_file = '../input/ml2020spring-hw4/testing_data.txt'\n",
    "\n",
    "w2v_path = '../input/w2v-model/w2v.model'\n",
    "model_path = 'hw4.model'\n",
    "\n",
    "sen_len = 20\n",
    "fix_embedding = True\n",
    "batch_size = 128\n",
    "epoch = 20\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    '''\n",
    "    class for reading data\n",
    "    '''\n",
    "    def __init__(self,train_file_label = 'data/training_label.txt',\n",
    "                 train_file_nolabel = 'data/training_nolabel.txt',\n",
    "                 test_file = 'data/testing_data.txt'):\n",
    "        self.train_file_label = train_file_label\n",
    "        self.train_file_nolabel = train_file_nolabel\n",
    "        self.test_file = test_file\n",
    "    \n",
    "    def train_data_label(self):\n",
    "        return self.load_train_data(self.train_file_label,labeled=True)\n",
    "\n",
    "    def train_data_nolabel(self):\n",
    "        return self.load_train_data(self.train_file_nolabel,labeled=False)\n",
    "\n",
    "    def test_data(self):\n",
    "        return self.load_test_data(self.test_file)\n",
    "    \n",
    "    def load_train_data(self,path,labeled=False):\n",
    "        with open(path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "            if labeled:\n",
    "                X = [line[2:] for line in lines]\n",
    "                Y = [line[0] for line in lines]\n",
    "                return X,Y\n",
    "            else:\n",
    "                X = lines\n",
    "                return X\n",
    "\n",
    "    def load_test_data(self,path):\n",
    "        with open(path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(',')[1:] for line in lines[1:]]\n",
    "            X = [''.join(line).split(' ') for line in lines]\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "\n",
    "dr = DataReader(train_file_label=train_file_label,train_file_nolabel=train_file_nolabel,test_file=test_file)\n",
    "X_train_label,Y_train_label = dr.train_data_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self,sentences,sen_len,w2v_path):\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.embedding = Word2Vec.load(w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def random_vector(self):\n",
    "        vector = torch.empty(1,self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        return vector\n",
    "        \n",
    "    def add_embedding(self,word):\n",
    "        # add word into embedding and give it random representation vector\n",
    "        # word will be '<PAD>' or '<UNK>' ONLY\n",
    "        vector = self.random_vector()\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(vector)\n",
    "        self.embedding_matrix = torch.cat((self.embedding_matrix,vector),0)\n",
    "        \n",
    "    def make_embedding(self):\n",
    "        print('Get embedding ...')\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "#             print(f'get words #{i+1}',end='\\r')\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        print(f'total words: {len(self.embedding_matrix)}')\n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def pad_sequence(self,sentence):\n",
    "        # make all sentences having the save length\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx['<PAD>'])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    \n",
    "    def sentence_word2idx(self):\n",
    "        # change words in sentence to idx\n",
    "        sentence_list = []\n",
    "        for i,sen in enumerate(self.sentences):\n",
    "#             print(f'sentence count #{i+1}', end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if word in self.word2idx.keys():\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx['<UNK>'])\n",
    "            # make all sentences having the same length\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def labels_to_tensor(self,labels):\n",
    "        y = [int(label) for label in labels]\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "preprocess = Preprocess(X_train_label,sen_len=sen_len,w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding()\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "train_y = preprocess.labels_to_tensor(Y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.data[index],self.label[index]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train valid set\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_x[:180000],train_x[180000:],train_y[:180000],train_y[180000:]\n",
    "# X_train,X_val,y_train,y_val = train_x[:18000],train_x[18000:20000],train_y[:18000],train_y[18000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "train_dataset = TwitterDataset(X_train,y_train)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "val_dataset = TwitterDataset(X_val,y_val)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model define\n",
    "\n",
    "class LSTM_Net(nn.Module):\n",
    "    '''\n",
    "    RRN\n",
    "    '''\n",
    "    def __init__(self,embedding,hidden_dim,num_layers,\n",
    "                dropout=0.5,fix_embedding=True):\n",
    "        super(LSTM_Net,self).__init__()\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # make embedding layer\n",
    "        self.embedding = nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        \n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(self.embedding_dim,hidden_dim,num_layers,batch_first=True)\n",
    "        \n",
    "        # define classifier, which is a fc nn\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim,150),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(150,150),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(150,150),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(150,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print('input data: ',type(x),x.shape)\n",
    "        inputs = self.embedding(x)\n",
    "        #print('after embedding: ',type(inputs),inputs.shape)\n",
    "        x, _ = self.lstm(inputs,None)\n",
    "        #print('after lstm: ',type(x),x.shape)\n",
    "        x = x[:,-1,:]\n",
    "        #print('after -1: ',x.shape)\n",
    "        out = self.classifier(x)\n",
    "        #print('after classifier: ', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_Net(embedding=embedding,hidden_dim=150,num_layers=1,dropout=0.5,fix_embedding=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trian method\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluation(output,labels):\n",
    "    '''\n",
    "    return the number of right predictions\n",
    "    '''\n",
    "    output[output>=0.5] = 1\n",
    "    output[output<0.5] = 0\n",
    "    return torch.sum(torch.eq(output,labels)).item()\n",
    "\n",
    "def training(batch_size,n_epoch,lr,train,valid,model,model_path,device,best_acc=0):\n",
    "    start = time.time()\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
    "    print(f'Parameters total num:{total} trainable:{trainable}')\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    t_len, v_len = (len(train),len(valid))\n",
    "    total_loss, total_acc = 0,0\n",
    "    \n",
    "    train_loss_history =[]\n",
    "    train_acc_history = []\n",
    "\n",
    "    val_loss_history =[]\n",
    "    val_acc_history = []\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss,total_acc = 0,0\n",
    "        for i,(inputs,labels) in enumerate(train):\n",
    "            inputs = inputs.to(device,dtype=torch.long)\n",
    "            labels = labels.to(device,dtype=torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            correct = evaluation(outputs,labels)\n",
    "            total_acc += (correct/batch_size)\n",
    "            total_loss += loss.item()\n",
    "            print('Epoch {} {}/{} acc {:.3f} loss {:.5f}'.format(epoch+1,i+1,t_len,\n",
    "                                                         correct*100/batch_size,total_loss),end='\\r')\n",
    "        print('\\nTrain total acc {:.3f} total loss {:.5f}'.format(total_acc*100/t_len,total_loss/t_len))\n",
    "        \n",
    "        train_loss_history.append(total_loss/t_len)\n",
    "        train_acc_history.append(total_acc*100/t_len)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0,0\n",
    "            for i,(inputs,labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device,dtype=torch.long)\n",
    "                labels = labels.to(device,dtype=torch.float)\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs,labels)\n",
    "                correct = evaluation(outputs,labels)\n",
    "                total_acc += (correct/batch_size)\n",
    "                total_loss += loss.item()\n",
    "            print('\\nValid total acc {:.3f} total loss {:.5f}'.format(total_acc*100/v_len,total_loss/v_len))\n",
    "            \n",
    "            if total_acc > best_acc:\n",
    "                best_acc = total_acc\n",
    "                torch.save(model,model_path)\n",
    "                print(f'Saving model with acc: {total_acc*100/v_len}')\n",
    "\n",
    "        val_loss_history.append(total_loss/v_len)\n",
    "        val_acc_history.append(total_acc*100/v_len)\n",
    "\n",
    "        print('-------------------------------------------------------------------------')\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(n_epoch),train_loss_history,range(n_epoch),val_loss_history)\n",
    "    plt.legend(['train','val'])\n",
    "    plt.title('loss')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(n_epoch),train_acc_history,range(n_epoch),val_acc_history)\n",
    "    plt.legend(['val','val'])\n",
    "    plt.title('acc')\n",
    "\n",
    "    print('======================================================================')\n",
    "    print(f'Best acc: {best_acc:.3f}')\n",
    "    print(f'Training used time: {time.time()-start:2.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "training(batch_size=batch_size,\n",
    "        n_epoch=epoch,\n",
    "        lr=lr,\n",
    "        train=train_loader,\n",
    "        valid=val_loader,\n",
    "        model=model,\n",
    "        model_path=model_path,\n",
    "        device=device,\n",
    "        best_acc=best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi-supervise\n",
    "# self-trainning\n",
    "\n",
    "X_train_nolabel = dr.train_data_nolabel()\n",
    "\n",
    "preprocess = Preprocess(X_train_nolabel,sen_len,w2v_path)\n",
    "embedding = preprocess.make_embedding()\n",
    "train_nolabel_x = preprocess.sentence_word2idx()\n",
    "\n",
    "\n",
    "# data loader\n",
    "train_nolabel_dataset = TwitterDataset(X=train_nolabel_x,y=None)\n",
    "train_nolabel_loader = DataLoader(train_nolabel_dataset,batch_size=batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "\n",
    "def getSemiSuperviseData(test_loader, model, device, threshold=0.8):\n",
    "    labeled_X = []\n",
    "    label = []\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(device,dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze().tolist()\n",
    "            for i in range(len(outputs)):\n",
    "                if outputs[i] >= threshold:\n",
    "                    label.append(1)\n",
    "                    labeled_X.append(inputs[i].cpu().tolist())\n",
    "                elif outputs[i] < 1 - threshold:\n",
    "                    label.append(0)\n",
    "                    labeled_X.append(inputs[i].cpu().tolist())\n",
    "#             break\n",
    "    return labeled_X, label\n",
    "\n",
    "# load model\n",
    "model = torch.load(model_path)\n",
    "\n",
    "labeled_X, label = getSemiSuperviseData(train_nolabel_loader,model,device,threshold=0.8)\n",
    "\n",
    "print(f'New labeled train data: {len(labeled_X)} label: {len(label)}')\n",
    "\n",
    "print('Train the model used new labeled data ...')\n",
    "\n",
    "\n",
    "train_newlabeled_dataset = TwitterDataset(torch.cat((torch.tensor(labeled_X),X_train),0),torch.cat((torch.tensor(label),y_train),0))\n",
    "train_newlabeled_loader = DataLoader(train_newlabeled_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "training(batch_size=batch_size,\n",
    "        n_epoch=epoch,\n",
    "        lr=lr,\n",
    "        train=train_newlabeled_loader,\n",
    "        valid=val_loader,\n",
    "        model=model,\n",
    "        model_path=model_path,\n",
    "        device=device,\n",
    "        best_acc=best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# Reading data\n",
    "X_test = dr.load_test_data(test_file)\n",
    "\n",
    "preprocess = Preprocess(X_test,sen_len,w2v_path)\n",
    "embedding = preprocess.make_embedding()\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "\n",
    "\n",
    "# data loader\n",
    "test_dataset = TwitterDataset(X=test_x,y=None)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "def testing(test_loader, model, device):\n",
    "    outputs_list = []\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(device,dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            outputs_list += outputs.int().tolist()\n",
    "    return outputs_list\n",
    "\n",
    "# load model\n",
    "model = torch.load(model_path)\n",
    "\n",
    "test_res = testing(test_loader,model,device)\n",
    "\n",
    "# saving results\n",
    "df = pd.DataFrame({'id':[i for i in range(len(test_x))],\n",
    "                  'label':test_res})\n",
    "\n",
    "print('saving result ...')\n",
    "df.to_csv('predict.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
