{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch\nimport time"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def model_para_num(model):\n    '''\n    返回当前模型参数量\n    '''\n    num_para = 0\n    for a in model.parameters():\n        num_para += a.flatten().shape[0]\n    return num_para"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"outputs":[],"source":"def readfile(path,label):\n    '''\n    path: path of images\n    label: bool, return y or not\n    '''\n    files = sorted(os.listdir(path))\n\n    X = np.zeros((len(files),128,128,3),dtype=np.uint8)\n    y = np.zeros((len(files)),dtype=np.uint8)\n\n    for i,file in enumerate(files):\n        img = cv2.imread(os.path.join(path,file))\n        X[i] = cv2.resize(img,(128,128))\n        if label:\n            y[i] = int(file.split(\"_\")[0])\n    if label:\n        return X,y\n    else:\n        return X"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"work_dir = '../input/ml2020spring-hw3/food-11/'"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"\nprint('Reading data')\n\nX_train, y_train = readfile(os.path.join(work_dir,'training'),True)\nprint(f'Size of training data {len(X_train)}')\n\nX_val, y_val = readfile(os.path.join(work_dir,'validation'),True)\nprint(f'Size of validation data {len(X_val)}')\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X_test = readfile(os.path.join(work_dir,'testing'),False)\nprint(f'Size of testing data {len(X_test)}')"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# from sklearn.model_selection import train_test_split"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# X_train_val = np.concatenate((X_train,X_val),axis=0)\n# y_train_val = np.concatenate((y_train,y_val),axis=0)\n\n# X_train,X_val,y_train,y_val = train_test_split(X_train_val,y_train_val,test_size=0.1,random_state=1)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"\nclass ImgDataSet(Dataset):\n    def __init__(self,X,y=None,transform=None):\n        self.X = X\n        self.y = y\n        if y is not None:\n            self.y = torch.LongTensor(y)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self,index):\n        x = self.X[index]\n        if self.transform is not None:\n            x = self.transform(x)\n        if self.y is not None:\n            y = self.y[index]\n            return x,y\n        else:\n            return x"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"batch_size = 128"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"train_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n])\n\ntrain_set = ImgDataSet(X_train,y_train,train_transform)\ntrain_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n\nval_set = ImgDataSet(X_val,y_val,test_transform)\nval_loader = DataLoader(val_set,batch_size=batch_size,shuffle=False)"},{"cell_type":"markdown","metadata":{},"source":"Model"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier,self).__init__()\n        # input [3,128,128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3,64,3,1,1),   # [64,128,128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),     # [64,64,64]\n            \n            nn.Conv2d(64,128,3,1,1), # [128,64,64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),     # [128,32,32]\n            \n            nn.Conv2d(128,256,3,1,1), # [256,32,32]\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [256,16,16]\n            \n            nn.Conv2d(256,512,3,1,1), # [512,16,16]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [512,8,8]\n\n            nn.Conv2d(512,512,3,1,1), # [512,8,8]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [512,4,4]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512*4*4,1024),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(1024,512),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(512,11)\n        )        \n        \n    def forward(self,x):\n        out = self.cnn(x)\n        out = out.view(out.shape[0],-1)\n        return self.fc(out)\n    \nmodel = Classifier()\n\nprint(model_para_num(model))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class Classifier_no_normal(nn.Module):\n    def __init__(self):\n        super(Classifier_no_normal,self).__init__()\n        # input [3,128,128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3,64,3,1,1),   # [64,128,128]\n#             nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),     # [64,64,64]\n            \n            nn.Conv2d(64,128,3,1,1), # [128,64,64]\n#             nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),     # [128,32,32]\n            \n            nn.Conv2d(128,256,3,1,1), # [256,32,32]\n#             nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [256,16,16]\n            \n            nn.Conv2d(256,512,3,1,1), # [512,16,16]\n#             nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [512,8,8]\n\n            nn.Conv2d(512,512,3,1,1), # [512,8,8]\n#             nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [512,4,4]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512*4*4,1024),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(1024,512),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(512,11)\n        )        \n        \n    def forward(self,x):\n        out = self.cnn(x)\n        out = out.view(out.shape[0],-1)\n        return self.fc(out)\n    \nmodel = Classifier_no_normal()\n\nprint(model_para_num(model))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# 层数减少一半，参数量差不多（13929974）\n\nclass Classifier_harf_deep(nn.Module):\n    def __init__(self):\n        super(Classifier_harf_deep,self).__init__()\n        # input [3,128,128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3,64,5,1,2),   # [64,128,128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),     # [64,64,64]\n            \n            nn.Conv2d(64,128,5,1,2), # [128,64,64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),     # [128,32,32]\n\n            nn.Conv2d(128,185,5,1,2), # [256,32,32]\n            nn.BatchNorm2d(185),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2,0),      # [256,16,16]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(185*16*16,256),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(256,11)\n        )        \n        \n    def forward(self,x):\n        out = self.cnn(x)\n        out = out.view(out.shape[0],-1)\n        return self.fc(out)\n\nmodel_harf_deep = Classifier_harf_deep()\n\nprint(model_para_num(model_harf_deep))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# DNN，参数量差不多（12749067）\n\nclass Classifier_dnn(nn.Module):\n    def __init__(self):\n        super(Classifier_dnn,self).__init__()\n        # input [3,128,128]\n        self.fc = nn.Sequential(\n            nn.Linear(3*128*128,256),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(256,256),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(256,256),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(256,128),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(128,11)\n        )        \n        \n    def forward(self,x):\n        out = x.view(-1,3*128*128)\n        return self.fc(out)\n\nmodel_dnn = Classifier_dnn()\n\nprint(model_para_num(model_dnn))"},{"cell_type":"markdown","metadata":{},"source":"Training"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def train(model):\n    loss = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n\n    best_acc = 0.0\n\n    train_acc_history = []\n    val_acc_history = []\n    train_loss_history = []\n    val_loss_history = []\n    \n    num_epoch = 30\n\n    for epoch in range(num_epoch):\n        start_time = time.time()\n        train_acc = 0.0\n        train_loss = 0.0\n        val_acc = 0.0\n        val_loss = 0.0\n\n        model.train()\n        for i,data in enumerate(train_loader):\n            optimizer.zero_grad()\n            train_pred = model(data[0].cuda())\n            batch_loss = loss(train_pred,data[1].cuda())\n            batch_loss.backward()\n            optimizer.step()\n\n            train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(),axis=1) == data[1].numpy())\n            train_loss += batch_loss.item()\n\n        model.eval()\n        with torch.no_grad():\n            for i,data in enumerate(val_loader):\n                val_pred = model(data[0].cuda())\n                batch_loss = loss(val_pred,data[1].cuda())\n\n                val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1) == data[1].numpy())\n                val_loss += batch_loss.item()\n\n            train_acc /= train_set.__len__()\n            train_loss /= train_set.__len__()\n            val_acc /= val_set.__len__()\n            val_loss /= val_set.__len__()\n\n            cost_time = time.time() - start_time\n            print(f'[{epoch+1:03d}/{num_epoch:03d}] {cost_time:2.2f} sec(s) Train Acc: {train_acc:3.6f} Loss: {train_loss:3.6f} | Val Acc: {val_acc:3.6f} Loss: {val_loss:3.6f}')\n\n            train_acc_history.append(train_acc)\n            train_loss_history.append(train_loss)\n            val_acc_history.append(val_acc)\n            val_loss_history.append(val_loss)\n\n            if val_acc > best_acc:\n                best_acc = val_acc\n                torch.save(model.state_dict(),'model_hw3_best.pth')\n                print(f'Better model found! Acc: {best_acc:3.6f}')\n                \n    num = len(train_acc_history)\n\n    plt.figure()\n    plt.plot(range(num),train_acc_history,range(num),val_acc_history)\n    plt.legend(['train','val'])\n    plt.title('acc')\n\n    plt.figure()\n    plt.plot(range(num),train_loss_history,range(num),val_loss_history)\n    plt.legend(['train','val'])\n    plt.title('loss')"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model = Classifier().cuda()\nprint(model_para_num(model))\ntrain(model)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model_no_normal = Classifier_no_normal().cuda()\nprint(model_para_num(model_no_normal))\ntrain(model_no_normal)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model_harf_deep = Classifier_harf_deep().cuda()\nprint(model_para_num(model_harf_deep))\ntrain(model_harf_deep)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model_dnn = Classifier_dnn().cuda()\nprint(model_para_num(model_dnn))\ntrain(model_dnn)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"train_transform = transforms.Compose([\n    transforms.ToPILImage(),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomRotation(15),\n    transforms.ToTensor(),\n])\n\ntrain_set = ImgDataSet(X_train,y_train,train_transform)\ntrain_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n\nval_set = ImgDataSet(X_val,y_val,test_transform)\nval_loader = DataLoader(val_set,batch_size=batch_size,shuffle=False)\n\nmodel = Classifier().cuda()\nprint('no data augmentation')\ntrain(model)"},{"cell_type":"markdown","metadata":{},"source":"从val set上看，这个模型表现是比较不错的。现在用所有的数据来train"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X_train_val = np.concatenate((X_train,X_val),axis=0)\ny_train_val = np.concatenate((y_train,y_val),axis=0)\n\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n])\n\ntrain_val_set = ImgDataSet(X_train_val,y_train_val,train_transform)\ntrain_val_loader = DataLoader(train_val_set,batch_size=batch_size,shuffle=True)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model = Classifier().cuda()\nloss = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n\ntrain_val_acc_history = []\ntrain_val_loss_history = []\n\nnum_epoch = 100\n\nfor epoch in range(num_epoch):\n    start_time = time.time()\n    train_acc = 0.0\n    train_loss = 0.0\n    \n    model.train()\n    for i,data in enumerate(train_val_loader):\n        optimizer.zero_grad()\n        train_pred = model(data[0].cuda())\n        batch_loss = loss(train_pred,data[1].cuda())\n        batch_loss.backward()\n        optimizer.step()\n        \n        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(),axis=1) == data[1].numpy())\n        train_loss += batch_loss.item()\n\n    train_acc /= train_val_set.__len__()\n    train_loss /= train_val_set.__len__()\n\n    cost_time = time.time() - start_time\n    print(f'[{epoch+1:03d}/{num_epoch:03d}] {cost_time:2.2f} sec(s) Train Acc: {train_acc:3.6f} Loss: {train_loss:3.6f}')\n\n    train_val_acc_history.append(train_acc)\n    train_val_loss_history.append(train_loss)        "},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"num = len(train_val_acc_history)\n\nplt.figure()\nplt.plot(range(num),train_val_acc_history)\nplt.title('acc')\n\nplt.figure()\nplt.plot(range(num),train_val_loss_history)\nplt.title('loss')"},{"cell_type":"markdown","metadata":{},"source":"Save model"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model_pth = 'model_hw3.pth'\n\ntorch.save(model.state_dict(),model_pth)"},{"cell_type":"markdown","metadata":{},"source":"Load model"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# model = Classifier().cuda()\n# model.load_state_dict(torch.load('../input/model/model_hw3.pth'))"},{"cell_type":"markdown","metadata":{},"source":"Testing"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"test_set = ImgDataSet(X_test,transform=test_transform)\ntest_loader = DataLoader(test_set,batch_size=batch_size,shuffle=False)\n\nmodel.eval()\nprediction = []\nwith torch.no_grad():\n    for i,data in enumerate(test_loader):\n        test_pred = model(data.cuda())\n        test_label = np.argmax(test_pred.cpu().data.numpy(),axis=1)\n        for y in test_label:\n            prediction.append(y)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"with open('predict_hw3.csv','w') as f:\n    f.write('Id,Category\\n')\n    for i,y in enumerate(prediction):\n        f.write(f'{i},{y}\\n')"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}